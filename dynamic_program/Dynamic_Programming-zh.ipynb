{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷你项目：动态规划\n",
    "\n",
    "在此 notebook 中，你将自己编写很多经典动态规划算法的实现。\n",
    "\n",
    "虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。\n",
    "\n",
    "### 第 0 部分：探索 FrozenLakeEnv\n",
    "\n",
    "请使用以下代码单元格创建 [FrozenLake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) 环境的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozenlake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体将会在 $4 \\times 4$ 网格世界中移动，状态编号如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体可以执行 4 个潜在动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，$\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$ 以及 $\\mathcal{A} = \\{0, 1, 2, 3\\}$。请通过运行以下代码单元格验证这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划假设智能体完全了解 MDP。我们已经修改了 `frozenlake.py` 文件以使智能体能够访问一步动态特性。 \n",
    "\n",
    "请执行以下代码单元格以返回特定状态和动作对应的一步动态特性。具体而言，当智能体在网格世界中以状态 1 向左移动时，`env.P[1][0]` 会返回每个潜在奖励的概率和下一个状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个条目的格式如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prob, next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "- `prob` 详细说明了相应的  (`next_state`, `reward`) 对的条件概率，以及\n",
    "- 如果 `next_state` 是终止状态，则 `done` 是 `True` ，否则是 `False`。\n",
    "\n",
    "因此，我们可以按照以下方式解析 `env.P[1][0]`：\n",
    "$$\n",
    "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
    "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
    "               0 \\text{ else}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "你可以随意更改上述代码单元格，以探索在其他（状态、动作）对下环境的行为是怎样的。\n",
    "\n",
    "### 第 1 部分：迭代策略评估\n",
    "\n",
    "在此部分，你将自己编写迭代策略评估的实现。\n",
    "\n",
    "你的算法应该有四个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `V`：这是一个一维numpy数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 在输入策略下的估算值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 4, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True)],\n",
       " 2: [(0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 5, 0.0, True),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 4, 0.0, False)]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    # V = np.zeros(env.nS)+np.inf\n",
    "    V = np.zeros(env.nS)\n",
    "    pas = np.zeros(env.nS)-1\n",
    "    ## TODO: complete the function\n",
    "    def dynamic_program(state):\n",
    "        print(\"state: {}\".format(state))\n",
    "        nonlocal env, policy, gamma, theta, V, pas\n",
    "        pas[state] = 1\n",
    "        for action, pi in enumerate(policy[state]):\n",
    "            E = 0\n",
    "            for p, next_state, r, done in env.P[state][action]:\n",
    "                if next_state == state:\n",
    "                    continue\n",
    "                \n",
    "                if pas[next_state] == -1 and V[next_state] == 0:\n",
    "                    dynamic_program(next_state)\n",
    "                next_record_v = V[next_state]\n",
    "                E += p*(r + gamma*next_record_v)\n",
    "            V[state] += pi*E\n",
    "    \n",
    "    for s in range(0, env.nS):\n",
    "        pas = np.zeros(env.nS)-1\n",
    "        dynamic_program(s)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将评估等概率随机策略  $\\pi$，其中对于所有 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s)$ ，$\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$。  \n",
    "\n",
    "请使用以下代码单元格在变量 `random_policy`中指定该策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "random_policy[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以评估等概率随机策略并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0\n",
      "state: 4\n",
      "state: 8\n",
      "state: 12\n",
      "state: 9\n",
      "state: 5\n",
      "state: 13\n",
      "state: 14\n",
      "state: 10\n",
      "state: 6\n",
      "state: 2\n",
      "state: 1\n",
      "state: 3\n",
      "state: 7\n",
      "state: 11\n",
      "state: 15\n",
      "state: 1\n",
      "state: 5\n",
      "state: 2\n",
      "state: 6\n",
      "state: 10\n",
      "state: 11\n",
      "state: 7\n",
      "state: 3\n",
      "state: 2\n",
      "state: 3\n",
      "state: 7\n",
      "state: 4\n",
      "state: 5\n",
      "state: 5\n",
      "state: 6\n",
      "state: 5\n",
      "state: 7\n",
      "state: 7\n",
      "state: 8\n",
      "state: 12\n",
      "state: 9\n",
      "state: 5\n",
      "state: 10\n",
      "state: 11\n",
      "state: 11\n",
      "state: 12\n",
      "state: 13\n",
      "state: 12\n",
      "state: 14\n",
      "state: 15\n",
      "state: 15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAFoCAYAAAD5IVjuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Hd8VFX+xvHPmUwaSQglEHoTBBEBSaS6C4pSbOv+LNhQd8UVd0XUtSCW3XUta1l1LYhgwcJaca3sKiCgQCyBBBEUkKLSDAlCSChJZs7vjxtIJpmEJJATIs/b130F7v3ec885M/PMzcwRY61FRERql6+uOyAiciRQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbOawYY9YbY06p637UBmPMxcaYj+q6H1I3FLb1nDHmRGPMImPMDmPMNmPMQmPMCcXHLjfGLKhGWx2MMdYY469hX241xnwSZn+SMabAGNOjJu0eCsaYacV9yCu1jarF65WbS2vtdGvtsNq6phzeFLb1mDGmIfA+8DjQBGgN/A3YW0ddegkYaIzpWGb/BcAya+3XddCn0h6w1saX2l6r4/7IEURhW78dDWCtfcVaG7DW7rbWfmSt/coYcwwwGRhQfBe3HcAYc7oxJsMYk2uM+dEY89dS7e27K91efM6A4nN+b4z5xhjzszHmQ2NM+3CdsdZuAD4GRpc5dCnwQnFbRxljPjbG5Bhjso0x040xjcK1V3w3enepvw8xxmwo9fdWxpgZxpitxph1xphrqzxzodexxpjO4a6775rGmD8bY7KMMZuNMb8rVRtrjPmnMeb74t8uFhhjYgkzl2V/0zDGDDTGfFl83pfGmIGljs0zxvy9+DeVncaYj4wxSTUZnxweFLb12yogYIx5wRgz0hjTeN8Ba+03wFggrfgubl+g5eOFXyPgdOBqY8zZxcd+XfyzUfE5acXHJgL/BzQDPgVeqaRPL1AqbI0xXYHepc4xwH1AK+AYoC3w1+oO3BjjA94DluLd0Q8FrjPGDK9uW1XQAkgsvs4VwJOl5vohIAUYiPfbxc1AkDBzWab/TYAPgMeApsDDwAfGmKalyi4Cfgc0B6KAGw/90MQVhW09Zq3NBU4ELDAV2GqMedcYk1zJOfOstcustUFr7Vd4ITi4kstcBdxnrf3GWlsE3Av0rujuFvgPkFzqLu1S4L/W2q3F1//OWjvLWru3eN/DB7h+RU4Amllr77LWFlhr1+LNwQWVnHOjMWZ78ZZdjWsVAndZawuttTOBPKBrceD/Hhhvrd1Y/NvFImttVT7GOR1Yba19yVpbZK19BfgWOLNUzfPW2lXW2t3A63hvWlJPKWzrueIQvNxa2wbogXfH+GhF9caYfsaYucW/eu/Au/ut7NfT9sC/9oUUsA3v7rS1MWZiqS+bJhf3ZxfwBnCpMcYAF1P8EULx9ZsbY141xmw0xuQCLx/g+pX1q1Wp8NyOdwde4RsN8JC1tlHxVp1r5hS/0eyzC4jH63cMsKa6ncd7nL4vs+97vLvnfbaEuabUUwrbXxBr7bfANLzQBe+Ot6x/A+8Cba21iXif65pK6n8ErioVUo2stbHFd3D3lvqyaWypc14AzgdOBRLwvsTb577i6/S01jYELil1/bLygQal/t6iTL/WlelXgrX2tAraqsyuSq5TmWxgD3BUmGMH+uf0NuG9YZTWDthYxWtLPaOwrceMMd2Kv7hpU/z3tsCFwGfFJT8BbYwxUaVOSwC2WWv3GGP64n0uuM9WvM8bO5XaNxm41RhzbPE1Eo0x5x2ga58C24EpwKvW2oIy18/D++KoNXBTJe1kAqcZY5oYY1oA15U69gWQa4y5pfhLqghjTA9TvOytmjKBi4rbGEEVP9aw1gaB54CHi7+siyj+Iiya8HNZ2kzgaGPMRcYYv/GWoXUn9I1JfkEUtvXbTqAf8LkxJh8vZL8G/lx8/GNgObCl1GeUfwTuMsbsBO7E+ywQ2P8RwD3AwuJfzftba/8D3A+8Wvxr/9fAyMo6Zb1/JPlFvDu3F8sc/hvQB9iB9wXRW5U09RLeF2DrgY+A/Uu1rLUBvM83ewPr8O4yn8H7Iqu6xhe3tR3vY4+3q3HujcAy4Eu8j1juB3zh5rL0SdbaHOAMvMcqB++LtTOstdX5LFnqEaN/PFxEpPbpzlZExAGFrYiIAwpbEREHFLYiIg4obEVEHKjWP6VnkpIsHTrUUldEROqh9eux2dkV/Y85+1Xv3y3t0AHS02vaJZFDJiJQ1z34ZQjqd9uDZk9IrVKdplpExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAXdhO2kSdOwIMTGQkgKfflp5/fz5Xl1MDHTqBJMnV7/NvXth3DhISoK4ODjrLNiwoeT40qVw4YXQti3ExkLXrvDggxAMhu/T6tWQkADx8dUb+6FSF3M4ZQqcdBI0agTGwPr15du45x4YNMibY2PKH6/uPNey4FOTCHTuSCAuhkDfFOwB5tHOn0+gb4pX36UTwafLz+OB2rRbthC8bDSB1i0INIwj0KcXwX9PLzk+bx4Bvwm72TffCG3rww8JDBpAIKEBgaaNCJw69CBmo2bspEnYTh2xsTHY1KrNoU1N8eqP6oQN81ysrE27bRt23DjsMd2wDWKx7dpir74am5MT2kbHDlifCd0mTAjfp+xsbJvWXk12dvUnobqstVXeSEmpenHp7dVXLX6/ZcoUy4oVlmuuscTFWb7/Pnz92rWWBg28uhUrvPP8fsubb1avzbFjLS1bWj76yLJ4sWXwYEuvXpaiIu/4s89axo2zzJ1rWbPG8sorlvh4yz33lO/T3r2WPn0sp53mXacm83AwW13N4SOPWO691/sJlnXryl/rjjssDz1kmTjRqyl7vDrzXMUtoqhmm/m3N2YzeYr1LVthzZ+8MfvWfh+23rfam0fzp2u8+snePPpef7NabXLKqZaUFOtb+Jn1rVpjzQMPWYyxvo/ne9fZtdf6NmwO2cwtt1ri461v+86S/sz4j6VRI2sef9L6ln/rXe/Fl2s+H8Hqb7xS/Lx5eopl+QpL8XhZ/334+jXFz8U/XePVP138XHzjzSq3yVfLLL/9reXtdyyrVlvmzrN072459dTQa7Vvb7njTsumzSVb7s7w/TrjDO/1DJasrTWaCxO0lpQUW6X8dBK2fftaxowJ3de5s2XChPD1N9/sHS+974orLP37V73N7dstkZGWl18uOf7DDxZjLP/7X8V9vekmL1TL7r/uOsvll1uef75uwrYu5rD09uWXFYftvu2NN8KHbXXmuYpbTcOFE/pac8WY0H2dO1tz84TwYXSjN48h+35/haVf/2q1SVycNc88F1rTrp019z9YcV+PPtqaMVeWBO3eIkvbttZMnlLj8R+SsO3b13LFmNB9nTtbbpkQvv4mbw5D9v3eey7WtE0TtJb3P/Bez9t3hIbtAw8eeAyPPGo5+WTL7DnOwrb2P0YoKIDFi2HYsND9w4bBokXhz0lLK18/fDikp0NhYdXaXLzYqy1d07YtHHNMxdcFyM2Fxo1D933wAbz/Pjz2WMXn1aa6msPaFG6ea5ktKIAlizGnho7ZnDoMmxZ+zPaztPL1w4bD4nRsYWHV2xx0IvaN17E5OdhgEPvuO7B1K2boKeGvO28erFqFGfOHkp2LF8OPP0J0NIET+ngfSYwYhs3IqPokHCRb0fPm1GFQwRzyWZp3vLTi5+L+Oaxum+A9h6KjoUGD0P3/fAib1BR7fG/sPfd47ZceQ0YGPHA/vPAi+Nx9klr7V8rOhkAAkpND9ycnw5Yt4c/ZsiV8fVGR115V2tyyBSIivM9rq3rdJUtg2jS4+uqSfZs3w5VXwksveZ/X1oW6msPaEm6eXdg35uZlxtw8GX6qYMw/bQlfX3YeD9Cm79XXwRiCyUkEG0QTHH0xvumvYHr3DntZ+8wU6NULk5pasm/dWu/nX+7AN2Eivnc/wLRpQ/DkwdhNm6o2BwfrMHku2u3b4c47YMyVGL+/5MC4a+Hfr8DHc+FP18Cjj8Af/1hyXn4+XHQhPPY4pnXrqo76kPAfuOQQKfvFibXhv0yprH7f/tJ/rk6bldWsXAmnnw7XXQfnnFOy/5JLvFDo37/ydl04XObwYFQ0zy7VwTzaO2+H7Gx8H86GpCTsO28TvPxSfHM/wfTqFXrqtm3Y/7yFeejh0DaLv1A0t96GOedcb9/kKdg5s7Evv4S5+ZaKx3Co1eFz0ebnw1lnQuvW8MADoZe54YaSv/TsiW3YEC4Yhb3/fkzTpnDttTBwEKYOnnu1f2eblOTdYZZ9h8rKKv9Otk+LFuHr/X5o2rRqbbZo4b1blv2WMdx1v/0WhgyBCy6Af/wj9NjHH8Pf/uZd2++HK66A/Hzvz1OmHHD4h0RdzeGhVtk8u7BvzGXvYrdmlb8z3Se5Rfj6svNYSZt2zRrsE4/je3oqZuhQTK9e+O78C6SegH3y8XKXtC++AD4f5qKLQ/abFi29n8d0L9nn90PnLvDjD1WYgEOgjp+LNi8PThvp/eW99zExMZX3t18/7+d333k/P54DL0zDRvqxkX44pXglR8sW2Ntuq7ytg1T7YRsV5S0pmjUrdP+sWTBwYPhzBgyA2bPL16emQmRk1dpMSfFqS9ds2ADffBN63RUrvAA47zx45JHyfVm2DDIzS7a77vKWL2Vmeue4UFdzeCgdaJ4dMFFR0CcFOzt0zHb2LMyA8GM2/Qdg58wuV09KKiYysmpt7trl/YyICG08IiLs8jf73DOY887HJCaGHkhJgeho7KqVJbXBIKxdA+3aVzTsQ8pU9LyZPQsqmEP6D4A54Z+L++ewCm3anTth5AjvJuqDmZiqLMHMzPR+tvTeqPjwI8hcChmZ3jb1GW//3HneXW9tqs6XwAe19Csy0jJ1qrfE6Npri5d1rPeOjx7tbfvq9y1bGj/eq5861Tu/7LKlytq01lv61aqVZdYsy5IlliFDQpd+ff21pXlzy6hRls2bQ7eKxlJXqxHqag43b7ZkZFimT/e+tf3gA+/vOTklNd9/7+178EGvJiPD23burPk819JqBPNvb8zm6anesqlx3ph9a9Z7xy8Zbc0lo8sv/bp2vFf/tDeP5ZZ+VdKmb3eB9836ib+yvkWfW9/K70qWfv3n3dClZvM+tYD1zV8Qvv/Xjre0bm19H/zP+pZ/6y0za9jQ+n7Y6HbpV2SkZcpUb5lW8XhZt947XvxcLLf069rxXv2U4udi2aVflbW5I9dbSdO9u2XlqtClXXv2ejULF1n++bBlSYZ3zVdf817/Z51V8Vg+nvsLW/plreXJJ71lGVFR3pKf+fNLjg0e7G2l6+fNsxx/vFffoYPlqaeq16a1lt27vbWjTZpYYmO9dXU//FBy/C9/8SY63Ha4hW1dzWFFc/T88yU1l10Wvmbu3JrPcy2FbUSRtebx0DHvW+saUWQtvx5s+fXg0ACcEzqP5smnqtVmRJG1vm9WWX77f96bToMGlp49rXluWvl2Rl9qOeaYCvvu211gzZ9vsiQnWxISLL8ebH1fLHa69MsEreWJMs+befNLjhU/F0Pq55Z5Lk56qnpt7gvFcNvHc72a9MWWfv0siYmWmBhL166WO/9iycs/LMLWWGurfBdsUlMt6ek1v40WOUQiAnXdg1+GoP6H/YNmT0jFpqcf8FtlTbWIiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg7467oDIjURn1fXPfhl2JFY1z04cujOVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERB9yF7aRJ0LEjxMRASgp8+mnl9fPne3UxMdCpE0yeXL02t22DceOgWzeIjYW2beHqqyEnJ7SNn3+G0aMhMdHbRo+G7dtLju/ZA5dfDj17QmQkDBlS0xk4fNTGY1GP7H1mErk9O7IjOYadg1MoWlT5+IsWzGfn4BR2JMeQ26sTe58LHf/eqU+yc2BPdrRtyI62Dck7dQCFH34QUrPr6svZ0ciEbHmn9A+tufZKdvY+ih0tYsk9qhn5F/6GwMpvQmoCmUvIP/tUdrRrRG7Hpuwa/wdsXt5BzEYdO5Kei9baKm+kpFS9uPT26qsWv98yZYplxQrLNddY4uIs338fvn7tWkuDBl7dihXeeX6/5c03q97msmWW3/7W8s47ltWrLfPmWbp3t5x6aui1Rozw9i9caFm0yPvzGWeUHM/Ls1x1leXppy2/+Y1l8OCazcHhstXGY1EHW+L2mm2xz3njj/3XFBv/+QobdaU3/oRl34etT8j0xh915TU2/vMVNvZf3vgbvPDm/poG09+2Dd6YaeOXrLbx6Stt9J8nWvx+G79g6f6ayAsvs/4hp9iElZtLtnU5IdeKeWSyjZv5iU1Yus7Gz1ts/SPOtKZlK9twa4HXl282WtOosY26/A82/stvbdzHX9iIfgOt/6xzajwfei4egi0lxValrFpt1jhs+/a1jBkTuq9zZ8uECeHrb77ZO1563xVXWPr3r3mb1lo++MBijGXHDu/vK1ZYwLJgQUnNp596+779tvz5f/pT/Q/b2ngs6mCrabhEpPS1kZeOCdnn69TZRl8/IWx91Pibra9T55B9kaOvsBEn9K/0OqZRYxvzyOTQsB1+erX6Gr9gqQVs/Jffem8Ujz5tTZOmtmFOUUnNwq+8miWr61/Y/kKei1UN29r/GKGgABYvhmHDQvcPGwaLFoU/Jy2tfP3w4ZCeDoWFNWsTIDcXoqOhQYOS68THw8CBJTWDBkFcXOXt1Fe18VjUI7aggEDmYiJPDh2P/+RhFH0efvyBL9Lwl6mPHDqcQEY6Nsz4bSBAwYxXsfl5+PsODDlWlLaA3M7N2ZlyNLuuvZLg1qyK+5qfT8H05zFt2uFr18Hbt3cvREZiIiJKCmNjvX6mLaiwrcPSEfhcrP2wzc6GQACSk0P3JyfDli3hz9myJXx9UZHXXk3a3L4d7rgDrrwS/P6S6zRrBsaU1BkDzZtX3E59VhuPRT1ic7zxm2ah4zHNkrFZ4cdvs7aEraeoyGuvWGD5Mna0jie3eTS7rx9Lg5f/Q8Sxx+0/7j9lBA0mv0jcO3OIufufBBZ/Qf5ZJ3sBWsreZyZ57bSOp2j2f4l7dw4mOtpr49cnY3Oy2fPIP7AFBdjtP7PnrxMACP60ueYTUxeOwOeiuy/ISgcagLXl9x2ovuz+qraZnw9nngmtW8MDD1R+nar0rb6rjceiPqmF8fu6dCX+00ziZn9G9BVXs/vqywis+Hr/8ahzLiDytLOIOPY4IkeeSdyb/yW4eiVFZb5IizrvYuI/ySDug/n4jjqaXZedh921C4CIY44l9qkXKHjqUXJbNiD36Bb42nfENE8OvdutT46g56K/1q+QlAQREeXfrbKyyr9L7dOiRfh6vx+aNvUmuKpt5uXBaad5f37/fe9bzNLXycoKfYCtha1bK+5bfVYbj0U9Ypp64y97F2uzs8rdve4/p3mLsPX4/ZgmJeM3UVFEdOoMgP/4VIqWfMneSY/Q4Ilnw7bra9kK06oNwbWrQ6+XmEhEYiIc1YWIE/qT26Exhe/OIOqC0QBEnXcRUeddRDDrJ0yDODCGgicfxte+Y/Umo64dgc/F2r+zjYrylmrMmhW6f9as0M9KSxswAGbPLl+fmuotv6pqmzt3wogR3q8rM2d6n8+WvU5envdZ0D5pad6dcEV9q89q47GoR0xUFBG9UyicGzr+ormz8PcLP/6IvgMomhc6/sK5s4g4PhVT2fiDQSjYW/HhnGzs5o2Y5JYVt2Gtt4Vpx9c8GRMfT+Fbr0FMDP4hp1bczuHoSHwuVudLt4Na+hUZaZk61VsBcO213hKP9eu946NHe9u++n1LPMaP9+qnTvXOL7v0q7I2c3O9bym7d7esWmXZvLlk27u3pJ0RIyw9eljS0rylXz16hC79stayfLklI8MyapQlJcX7c0ZG3X4DWtOtNh6LOtgOaulXZKSNfWyqt/TrKm/8CV+t91YNjBptI0eNLr/0a+x4b+nXY974Sy/9ir7ulpIlWwu/stHXT7AYYxu8MdMmbre24YadNuqaP9u4jxbZhKXrbNx7c23ECf2tadXaNvwx11tVsGS1jfnrP2z8vHSbsOx7G/fhQusfcaYlsZFNWLm5ZHnYA4/b+HmLbXz6Shvz4BOW2Fgb849/1d+lX7+A5+LhtfTLWsuTT1rat7dERVn69LHMn19ybPDg8kuq5s2zHH+8V9+hg+Wpp6rX5ty53hKucNvcuSV1OTmWiy+2JCR428UXW37+OfQ67duHb6cuH+CD2WrjsXC81TRcErdbG/PQk9a09cbv69XHxn0wv2Rp2KDBNmLQ4JD6uPfnWV9Pb/ymXQcb8/BToUvBLrzMmjbtvONJzWzE4KG2wYz/7T/ecPMu6z95mDVJzSyRkda0aWcjL7zMJnz9Q0mof/2D9Z8yoqSmdRsbed5FNv6Lb0KvNWq0NY2beH0/tqeNnfziQc1FXT+Ov4TnYlXD1lhrq3wXbFJTLenph/rmWqTaEnfUdQ9+GXYk1nUPfgFSU7Hp6Qf8hk7/NoKIiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREH/NU9ISJQG904sjTNqese1H/HLavrHvwyzBla1z04cujOVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERB5yFbfCpSQQ6dyQQF0Ogbwr2008rrbfz5xPom+LVd+lE8OnJ1W7TbtlC8LLRBFq3INAwjkCfXgT/Pb3keDBI4OyzCHRs57XRpiXBSy/BbtxYUrN0KcGLLyTQoS2B+FgC3bsSfOhBbDB4kDNSfbuen0R2akey2sWw7dQUCj6rfA4LFs1n26kpZLWLIfuETux+ofwc7pP/6L1kJRt23npNyP5g1k/kXns52T1bkdWhAdsvGEHR2tUhNUXr17D98t+ytXszth7VkB1Xnk8w66f9x20wyPbRZ5Hdp53Xl+NasuOPlxDYvJG6sOGdSSy8pCNzR8bwxdUp/Lys4nncm7OZr++5iLTfdWPOsAhWPHB5uZpNH05jzimm3BYo2FPl6xbmbmPl4+NI+1035p4Wy4IL2/Lto1dTuCMnbL8CBXv4/A+9mHOKIXdles0m4nAwaRJ07AgxMZCSAgfIBebP9+piYqBTJ5hc8XP6cOMkbIOvv4a9fjxmwkR86RmYAQMJnjES+8MPYevtunUEzzwNM2CgV3/Lrdjx47BvzahWm8HLL8V++w2+t97Bl7kMc8ml2MtGYz/5ZH+NOelkfK+8jm/FSnyvz8CuXUvwnN+W9GXJYmjWDN+0l/B9tRzzl79h774Le/8/amGmKrbn7dfIu308ceMn0mR2BpGpA9lx4UgCG8LPYeD7dWy/6DQiUwfSZHYGcdfeys6J49jz/oxytYXpn7H75an4u/cM2W+tZfvlZxNYu5rEaW/TZHYGvjbt2X7eKdj8fK8mP5/t5w8Da2n8xhwav7cQW1DA9tFnhrwhRZ14MolTXqfpwpUkPjeDwPdr2XH5b3Htp7mvsWrSeDpcOJG+kzNI7D6QpbeOZM9P4ecxWLiXyMQk2l8wgcRu/Sps1xfTgBNf3xyyRUTFVPm6e3M2sTdnI52vfIB+U5dx7ISX2b7sE76+98Kw1/vu6RuJTmpzEDNxGHjtNRg/HiZOhIwMGDgQRo6ECnKBdevgtNO8uowMuPVWGDcOZpR/Th+OjLW26sWpqTbi8+q/iwYG9MP07Inv6akl+7p1wfzfufjuva9cfXDCLdi33yLi25I7qOAfxmCXLydiYVqV2wwkxmP+9Ti+y39XUtOpPeZP4/D9+cawfbXvvUvwt7/Bl7cbExMTtiZ4y83YuXOI+GJxNWahRNPwNyuV2jaiH/7uPWn4cMl4c/p3IfqMc4m/vfwc5v39FvZ+8BZNPyuZw9zrx1C0cjlNZqbt3xfM3cG2U/rQ8J9TyX/4LvzdepBw3xMAFK1ZxbaBXWn8cSaRx/YCvLvU7B4tiJ94L7GXjGHvvI/YccEIkr7Nwdeo8f42s49uTKPXPiJq8Clhx7P3f++y47Lf0Oz7iue5Msctq/YpAHx5TT/iO/bkmD+XzOOiy7rQ/Ffn0nlM+XksLfO2M4hKTKL7zdNC9m/6cBqrHr+GIe/nHdLrZn9HsmbKAAAVzUlEQVQ+k6W3n8Hgt7fjj2u4f//Whe+w5rmJHHfnm3x2RXdOePJLGnZNrbTvFZkztEanHRr9+kHPnjC1ZE7o0gXOPRfuCzMnt9wCb70Fq0v9ZjVmDCxfDmlp5etdSU3FpqebA5XV+p2tLSiAJYsxpw4L2W9OHYZNWxT+nM/SytcPGw6L07GFhVVvc9CJ2Ddex+bkYINB7LvvwNatmKHhA8Bu24b993To26/yANiZC8XB4oItKKDoq8VEDQkdb9SQYRSmh5/DwvS08vUnDadoqTeH++z88x+IOfNcon51cvlG9u4FwESXzIXx+TDR0RR+saCkxpjQmugY8Pko2FdTRvDnbeyZMR1/nwPM8yEWLCxg56rFNEkNnZcmKcPYsSL8PFZVoGA3Cy9qz4IL2pB52xnsXJ1x0Nct2pWLLzIaX0yD/fv2bN3At49dzbG3TscXHXtQfa5TBQWweDEMC50Thg2DRRXMSVpa+frhwyE9HUo9pw9Xtf8xQnY2BALQPDl0f/Nk+GlL+HN+2hK+vqjIa6+KbfpefR2MIZicRLBBNMHRF+Ob/gqmd++Q04ITbiHQMI5g86bYH37A9+77FQ7HLlmCfWEavrFXH3Doh0pwmzdeX7PQ8fqaJRPMCj+HwawtYespKvLaA3a/NJXA+u+Iu+XvYduI6NINX9v25N87keDP27AFBeQ/fj/BTRsI/rQZgMiU/pi4ePLuugmbn4/NzyfvrzdCILC/Zp+8v99CVoc4srs1JbDxBxq9XPE814bCHdnYYICoxqHzEtU4mYJtFTwXqyCubVe63/gcPe96h2NvewVfVAzp1w1i14bVNb5uYd521k67g1anXYkvwg+ADQRYft/FtDv3zyR07h32vHpj32s4ucxrODkZtlTwWGzZEr5+Xy4c5tytRjBl7rKtLb/vQPVl9x+gTXvn7ZCdje/D2fg+T8f8+Sbvc9ylS0MvdeNN+NIz8P33I4iI8L4kC/Pxil25kuBZp2OuvQ7zf+dU3PfacgjnsOi7leTdN5GGk6ZjoqLCnx4ZSeKzMwisX0N2t6Zs7dCAwoVziRo6EiIiAPAlNaPhM2+wd85/2XpUAlu7JBLM3Y6/Zx9Mcc0+Df54E03mZNDo9Y8wERHk/in8PNe66s7jASR2H0DLYZeR0Lk3jY/7Fcfd/hqxLY/ix7cfr9F1A7vzWXr7mUQ3bU3nPzywf//6V+7FRETS7twbatzXw05t5MJhyl/rV0hK8l6YZe9it2aVvzPdJ7lF+Hq/H5o29Sb4AG3aNWuwTzyOb3Emppf3eaPp1YvAgk+xTz6OmfLM/tNMUpLXz6OPxnfMMQQ7tIUFC+BXv9pfY7/9luApJ2HOvwDffW6/HPM18eaw7F1sMDur3N3r/nOatwhbj9+Pr3FT9nz8P2xONtsG9ygpCAQoTPuE3S9Mptm6fEx0NJG9UmjycSbB3B1QUIAvqRnbRvQjsnfJZ4TRQ4YR/cUagjnZXvuJjcju0QJfu46hfWqahK9pEhx1NBFdjiHn+LYUfr6AqP6/woXIxCSML6Lc3WTB9qxyd50Hw0RE0LBrKrs3rq72dYt257F04mkA9Lrn/ZAv2bYtmcP2rz9l7vDIkHPSx/Wn+ZBR9Jg4nXpjXy6UvYvNyip/97pPixbh6/flwmGu1u9sTVQU9EnBzp4Vst/OnoUZMDD8Of0HYOfMLldPSiomMrJqbe7a5f0sc3dFRARUtmxr37HizysB7IoVBIcOwZx7Hr6HH6n43FpioqLw90yhYH7oeAvmzyIyNfwcRqYOoOCT2eXq/b28OYweeTZN5i2jyZzM/Zu/dyrRZ19AkzmZUOZu19cwEV9SM4rWrqZoaTpRI35T7pq+pkn4EhtR8OnHBLOziB5+VsWDCjPPtc0XGUXC0SlsWxw6j9sWzyKxe/h5rAlrLXlrvyKqSctqXbdo104yJ4zABgP0vncm/tj4kPruNz1Pv6eX0vfpTPo+nUmve2cCcOyt0+l85f2HrP9OREV5S7hmhc4Js2Z5qw3CGTAAZs8uX5+aCpGR4c85jNT+nS1grr8Be9logif0xQwchJ0yGTZtwlw1FvCWaAH4pr3o1V81FjvpCYI3XIe58irsooXe56TTX6lym3TrBp07E7zmj/geeAiaNsW+8zbMnoV56x0AbFoaNmMJZtCJ0KgRrFlD8C93QIcOcOKJXs3y5QRPPRkz5CTMhInYUu+spkWL2p66/RqMvYHca0YTeXxfIvsOYvcLkwlu2UTsZd54c6/x5rDhE94cxl46ll3PPsHO268j9tKrKPxiIXtem0bDyd4c+hIb4UtsFHIN0yAOX+Mm+I8pudvd8+4b+JokEdGmPUXfLGPnHeOJHnk20aW+fNv9yvP4O3fDNGtOUXoaO28fT+xV1+Pv3BWAwi/TKFy2hMh+J+Jr2IjA+jXk338HvrYdiOx3Yu1NWhjtzrmB5fePpmG3vjQ6dhAb3p9MQc4mWp/pzePyf3jzeOyEF/efs/O7TAACu3Ip9PnY+V0mJjKK+PbdAVj74t9IPKY/DVp3oWhXLj/+5zHy1n5F1/FPVfm6Rbt2knHLMAK7cun5t7cJ7MknsMdbXheZ0ARfZBSxLUN/U4goDuPYVkcR06weLgO74QYYPRr69oVBg7w1s5s2wdji1/Cl3mPBi8WPxdix8MQTcN11cNVVsHAhTJsGr7wStvnDjZOw9Z0/imBODvbeu7GbN0OPHvjem4lp3x6g3Hpb07EjvvdmErzxeuzkp6BVK8yjj4V8TnqgNk1kpNfGxAkEzz4T8vKgc2fMs89jzjzTayQ2FjvjTexf7/SOt2qFGT4C88pr+78lt2++AVlZ2Ndfw77+Wkg/I4rcfd4Yc/Yogj/nkP/o3QR/2oy/Ww8S/z2TiLbeeAMbQ+cwon1HGv17Jnl3Xs/uF57Cl9yKhHseI+aM6n3WHPxpM3l/uYHg1p/wJbck5rxLibvhjpCawHcryb/nVoLbtxHRtgNx191G7FXXlxTExrL3vTfJf+BObH4evuRWRJ88goZTXnO6GgEg+aRRFObmsH763ezdtpn4Dj3ode9MYpO9edyTVX6N5xdjjw/5e3bae8Qkt2fQ9PUAFOVt59tH/sDen7fgj0sk4ajjSXnkExK79a3ydXeuWkzuN58BkHb50SHX6/PQXBr3HnKopuDwMWoU5OTA3XdD8WuYmTOh+DVcbr1tx47e8euvh6e8XOCxx+CcOvj+pAacrLOVUDVZZyuharrOVkLV6TrbX4rDZZ2tiIgobEVEnFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDjgr+4JgYja6MaR5eiVdd2D+m/2KXXdg18GY+u6B0cO3dmKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuJA/QrbSZOgY0eIiYGUFPj008rr58/36mJioFMnmDzZTT9ryeb/TOLL8zuy8JQYMsaksGNpxeMvyN7MyrsuYvEl3VgwJIJV915eriZ77htkXplK2mmNWDQsjozf9+an/74QUvPl+R1Y8GtTblt+8+lhr/vjS/ey4NeGNY9cE7I/sCuPNY+O44tz2rDolFgWX9yVja8/Uv1JOAQmMYmOdCSGGFJI4VMqnsd5zMOE+e9bvt1fM4QhYWuO5dj9NctZzrmcSyc6YTD8lb+Wu9ZOdnId19Ge9sQSy0AG8iVfhtTcwR10oxtxxNGYxgxlKItYdPCTUleOoNd0/Qnb116D8eNh4kTIyICBA2HkSPjhh/D169bBaad5dRkZcOutMG4czJjhtt+HyNY5r7H2sfG0HT2R45/JoGGPgSy/eSR7fgo//mDhXvyJSbS5eAIJx/QLW+Nv2JS2l95Or6c+4/jnv6L5yN+x+oEr2JY2c39N7ylf0vc/m/dvvZ9ZAsaQdNL55drLXf4ZW96bSoOjepY7tvbJG9iW9gFH3/YSfV76hjajb2P90xPI+vClGs5IzbzGa4xnPBOZSAYZDGQgIxnJD1TwPCq2nOVsLvVfF7rsP/YWb4UcW896EkjgfErmaBe76EAH7uZuOtIx7DXGMIYP+ZAXeIFlLGMYwziFU9jIxv01XenKkzzJMpaxgAV0pCMjGMFP/HSQM1MHjrDXtLHWVr04NdWSnl6L3alEv37QsydMnVqyr0sXOPdcuO++8vW33AJvvQWrV5fsGzMGli+HtLTa728lTjzAm3c4mVf1I+6onnS5uWT86Rd2IWnIuXS4Ksz4S1l+yxlEJiZx9MRpB7xOxhV9aNx3eIVt/vjiPWx49UH6vrWJiJgG+/cX5e0gc0wfOt80lR9euIu4jj046von9h9fclkPmg4+h/a//9v+fV+NG0xcp+NC6qrq019X+xQA+tGPnvRkKiXz2IUunMu53Ef5Mc9jHidxElvZShJJVbrGdKZzKZeynvW0pW254z3owbmcG3J3u5vdJJDADGbwG36zf38KKYxkJHdzd9hr5ZJLIon8j/8xnOFV6l9ppuov/0Pvl/KaTk3FpqebA5XVjzvbggJYvBiGDQvdP2wYLKrgV6i0tPL1w4dDejoUFtZOP2tJsLCAvFWLaXxC6HganzCM3K8Pza+Q1lq2L57D7h9X0rBX+CSz1rLlg2dpfuolIUEL8N2Df6Dp4HNplHJy2HMbHnci2xa+x96ffgQgd9ki8r/LpHG/EYek/1VRQAGLWcwwQudxGMMO+Kt4Kqm0pCVDGcpc5lZaO5WpjGRk2KCtSBFFBAgQQ0zI/lhiWcCCsOcUUMAUptCQhvSmd5WvdVg4Al/T9SNss7MhEIDk5ND9ycmwZUv4c7ZsCV9fVOS1V48U7vDGH9k4dDyRTZIp3FbB+KuoKG8Hi4bHs+jkKJbfcjqdrn2MJv1Hhq3d/uUs9m5eR/IZY0L2b3lvKrs3fkf7MX+v8Dqdxj9GXJfefHleOxaeFMmyawfTYez9NBl4xkH1vzqyySZAgGRC5zGZZLYQfh5b0pKneIoZzOAt3qIrXRnKUD7hk7D1q1jFfOZzJVdWq28JJDCAAdzN3WxkIwECvMzLpJHGZjaH1L7P+8QTTwwxPMIjzGJWuTEd9o7A17S/rjtQLabMnbq15fcdqD7c/vqiuuOvgogGCRz/bCaB3XlsXzyHdU/eQEzLDjRKGVqudsv7U4nvdgLxXUruonb9sJL1UybS84lP8UVGVXidTTMeZ+eyhRxz37vEtGjPjsxPWDfpRmJadHB6dwtgCJ0ziy23b5+uxf/tM4ABrGc9D/EQv6b8bwBTmUpLWnI64b9ArMxLvMTv+T1taEMEEfShDxdyIUtYElJ3EieRSSbZZDOVqZzP+aSRRktaVvuade4Iek3Xj7BNSoKIiPLveFlZ5d/p9mnRIny93w9Nm9ZOP2tJZKI3/rJ3sYU/Z5W7260u4/MR26YzAPFderP7+2/48aV7y4Vtwc9ZbFvwDkdd/2TI/p3L0yjakc2Sy3uU7AwEyF36CZvfnczAD/OxNsj3U26l29/eoOmgMwGIO6on+d9lsuHVh5yFbRJJRBBR7i42i6xq3Rn2ox+v8mq5/QUU8AIvcCVX4q/BS+sojmI+88knn1xyaUlLRjGq3BdqccTRufi//vSnC114hme4gzuqfc06cwS+puvHxwhRUd5yj1mzQvfPmuV9MxnOgAEwe3b5+tRUiIysnX7WEl9kFPFHp/Bzeuj4f06fRcMeFYy/hqwNYgv3ltuf9d9p+CKjSRp6Qcj+pieezfHTlnH8s5n7t/huqTQbegHHP5uJiYzCFhViiwoxEREh5xpfBASDh7T/lYkiihRSmEXoPM5iFgOp+jxmkhn2LvJt3iabbK7gioPqZxxxtKQlP/MzH/JhyBdm4QQJspfyj9lh7Qh8TdePO1uAG26A0aOhb18YNMhbX7dpE4wd6x2/9FLv54svej/HjoUnnoDrroOrroKFC2HaNHjllTrp/sFqff4NrLpnNAnH9KVhj0FsfmcyBTmbaPEbb/wr7/HG3/W2F/efk7c6E4BAfi7G+MhbnYkvMooGHboD3sqChO79iGnViWDBXrZ9NpOtH75Ep+seD7m2tZYt7z9D0tAL8DdICDnmT2iEP6FRyD5fTBz+hCbEdfLudv1xDWnYezDrn56ALzaemOT27Fg6n6wPX6TD1Q8cwlk6sBu4gdGMpi99GcQgJjOZTWxiLN48Xoo3jy/izeOjPEoHOnAsx1JAAS/zMm/zNjMov9xoClMYylA60ancsQIKWMEKAPawhy1sIZNM4omnM95vFh/yIUGCdKMb3/EdN3ETXenK7/gd4K08eIAHOJMzaUlLtrKVJ3mSDWwIWWZWbxxhr+n6E7ajRkFODtx9N2zeDD16wMyZ0L69d7zs2ryOHb3j118PTz0FrVrBY4/BOee47/sh0GzoKIpyc/jxxbspyNlMg449OPb+mcS08Ma/N8x628wrjg/5+7ZF7xHdoj0nvL4egMDuPL7759UUbN2ALzqW2HbdOPq2F2l2yoUh5+3ImMeeDavpevvLNe5/t7+8yvopt7Lq7xdTlLuN6BbtaXfF32n5f9cc+ORDaBSjyCGHu7mbzWymBz2YyUza481j2fW2BRRwIzeykY3EEsuxHMsHfMBpnBZSt5a1fMzHYT9eANjEJo6n5PFYwxqe5mkGM5h5zANgBzu4lVvZwAaa0IRzOId7uIdIvLs2P36Ws5zneI4ccmhKU07gBD7hE3pSfm3zYe8Ie03Xn3W2vyA1WWcroWq6zlZC1ek621+KX9Q6WxGRek5hKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAGFrYiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgDClsREQcUtiIiDihsRUQcUNiKiDigsBURcUBhKyLigMJWRMQBha2IiAMKWxERBxS2IiIOKGxFRBxQ2IqIOKCwFRFxQGErIuKAwlZExAFjra16sTFbge9rrzsiIvVOe2ttswMVVStsRUSkZvQxgoiIAwpbEREHFLYiIg4obEVEHFDYiog4oLAVEXFAYSsi4oDCVkTEAYWtiIgD/w+Aq+1z4d8UkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1cbad8240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plot_utils import plot_values\n",
    "\n",
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保你的 `policy_evaluation` 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0\n",
      "state: 4\n",
      "state: 8\n",
      "state: 12\n",
      "state: 9\n",
      "state: 5\n",
      "state: 13\n",
      "state: 14\n",
      "state: 10\n",
      "state: 6\n",
      "state: 2\n",
      "state: 1\n",
      "state: 3\n",
      "state: 7\n",
      "state: 11\n",
      "state: 15\n",
      "state: 1\n",
      "state: 5\n",
      "state: 2\n",
      "state: 6\n",
      "state: 10\n",
      "state: 11\n",
      "state: 7\n",
      "state: 3\n",
      "state: 2\n",
      "state: 3\n",
      "state: 7\n",
      "state: 4\n",
      "state: 5\n",
      "state: 5\n",
      "state: 6\n",
      "state: 5\n",
      "state: 7\n",
      "state: 7\n",
      "state: 8\n",
      "state: 12\n",
      "state: 9\n",
      "state: 5\n",
      "state: 10\n",
      "state: 11\n",
      "state: 11\n",
      "state: 12\n",
      "state: 13\n",
      "state: 12\n",
      "state: 14\n",
      "state: 15\n",
      "state: 15\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**<span style=\"color: red;\">PLEASE TRY AGAIN</span>**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import check_test\n",
    "\n",
    "check_test.run_check('policy_evaluation_check', policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 2 部分：通过 $v_\\pi$ 获取 $q_\\pi$\n",
    "\n",
    "在此部分，你将编写一个函数，该函数的输入是状态值函数估值以及一些状态 $s\\in\\mathcal{S}$。它会返回输入状态 $s\\in\\mathcal{S}$ 对应的**动作值函数中的行**。即你的函数应同时接受输入 $v_\\pi$ 和 $s$，并针对所有 $a\\in\\mathcal{A}(s)$ 返回 $q_\\pi(s,a)$。\n",
    "\n",
    "你的算法应该有四个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `s`：这是环境中的状态对应的整数。它应该是在 `0` 到 `(env.nS)-1`（含）之间的值。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "\n",
    "- `q`：这是一个一维 numpy 数组，其中 `q.shape[0]` 等于动作数量 (`env.nA`)。`q[a]` 包含状态 `s` 和动作 `a` 的（估算）值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    q = np.zeros(env.nA)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请运行以下代码单元格以输出上述状态值函数对应的动作值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.nS, env.nA])\n",
    "for s in range(env.nS):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `q_from_v` 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('q_from_v_check', q_from_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 3 部分：策略改进\n",
    "\n",
    "在此部分，你将自己编写策略改进实现。 \n",
    "\n",
    "你的算法应该有三个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "\n",
    "请完成以下代码单元格中的函数。建议你使用你在上文实现的 `q_from_v` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `policy_improvement` 函数满足上文列出的要求（具有三个输入、一个输出，并且没有更改输入参数的默认值）。\n",
    "\n",
    "在继续转到该 notebook 的下个部分之前，强烈建议你参阅 **Dynamic_Programming_Solution.ipynb** 中的解决方案。该函数有很多正确的实现方式！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('policy_improvement_check', policy_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 4 部分：策略迭代\n",
    "\n",
    "在此部分，你将自己编写策略迭代的实现。该算法会返回最优策略，以及相应的状态值函数。\n",
    "\n",
    "你的算法应该有三个**输入**参数：\n",
    "\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 `policy_evaluation` 和 `policy_improvement` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def policy_iteration(env, gamma=1, theta=1e-8):\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。最优状态值函数已调整形状，以匹配网格世界的形状。\n",
    "\n",
    "**将该最优状态值函数与此 notebook 第 1 部分的状态值函数进行比较**。_最优状态值函数一直都大于或等于等概率随机策略的状态值函数吗？_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the optimal policy and optimal state-value function\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_pi,\"\\n\")\n",
    "\n",
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `policy_iteratio` 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('policy_iteration_check', policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 5 部分：截断策略迭代\n",
    "\n",
    "在此部分，你将自己编写截断策略迭代的实现。  \n",
    "\n",
    "首先，你将实现截断策略评估。你的算法应该有五个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `max_it`：这是一个正整数，对应的是经历状态空间的次数（默认值为：`1`）。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1):\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，你将实现截断策略迭代。你的算法应该接受四个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `max_it`：这是一个正整数，对应的是经历状态空间的次数（默认值为：`1`）。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正整数，用作停止条件（默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。\n",
    "\n",
    "请实验不同的 `max_it` 参数值。始终都能获得最优状态值函数吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_tpi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_tpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `truncated_policy_iteration` 函数满足上文列出的要求（具有四个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('truncated_policy_iteration_check', truncated_policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 6 部分：值迭代\n",
    "\n",
    "在此部分，你将自己编写值迭代的实现。\n",
    "\n",
    "你的算法应该接受三个输入参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。 \n",
    "- `theta`：这是一个非常小的正整数，用作停止条件（默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `truncated_policy_iteration` 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('value_iteration_check', value_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
